"""
í•œêµ­ì–´ -> ì˜ì–´ ë²ˆì—­ + FLUX ì´ë¯¸ì§€ ìƒì„± API (Render FastAPI â†” ngrok ë¡œì»¬ ëª¨ë¸/ComfyUI)

ì „ì œ ì¡°ê±´:
- FastAPIëŠ” Renderì—ì„œ êµ¬ë™(ë©”ëª¨ë¦¬ 512MB ì œí•œ)
- ë²ˆì—­ ëª¨ë¸ê³¼ ComfyUIëŠ” "ë¡œì»¬"ì—ì„œ ì‹¤í–‰í•˜ê³  ngrokìœ¼ë¡œ ê³µê°œ
  - Render FastAPIëŠ” ì•„ë˜ ë‘ ì£¼ì†Œë¡œë§Œ ë¶™ìŒ:
    1) TRANSLATION_BRIDGE_URL  (ë¡œì»¬ ë²ˆì—­ ë¸Œë¦¿ì§€ ngrok)
    2) COMFYUI_URL             (ë¡œì»¬ ComfyUI ngrok)

ë¡œì»¬ ì¸¡ ì¤€ë¹„:
1) ComfyUI (ì˜ˆ: 8188) ì‹¤í–‰ í›„ ngrok http 8188
   â†’ ì˜ˆ: https://comfy-xxxx.ngrok-free.app

2) ë²ˆì—­ ë¸Œë¦¿ì§€(ê°„ë‹¨ FastAPI) ì‹¤í–‰ í›„ ngrok http 7000 (ì˜ˆì‹œ)
   - /translate:  POST { "text": "..." } â†’ { "text": "..." (ì˜ì–´) }
   - /classify :  POST { "text": "...", "type": "person"|"object" } â†’ { "yes": true/false }
   â†’ ì˜ˆ: https://trans-xxxx.ngrok-free.app
   (ë²ˆì—­ ëª¨ë¸ llama.cppë¥¼ ì´ ë¸Œë¦¿ì§€ì—ì„œ ë¡œë“œí•˜ì„¸ìš”. Renderì—ì„œëŠ” ì ˆëŒ€ ëª¨ë¸ ë¡œë“œ X)

Render ì¸¡(ì´ íŒŒì¼):
- ì•„ë˜ TRANSLATION_BRIDGE_URL, COMFYUI_URL ì„ ngrok ì£¼ì†Œë¡œ ì„¤ì •
- BACKEND_PUBLIC_URLì€ Render ë„ë©”ì¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ /static/* ê²½ë¡œë¡œ ì—…ë¡œë“œ íŒŒì¼ ì ‘ê·¼ ê°€ëŠ¥

ì‚¬ìš©ë²•:
1) (ë¡œì»¬) ComfyUI + ë²ˆì—­ë¸Œë¦¿ì§€ ì‹¤í–‰ í›„ ngrokìœ¼ë¡œ ê³µê°œ
2) (Render) FastAPI ì„œë²„ ì •ìƒ êµ¬ë™
3) POST /image-from-copy
   { "text":"í•˜ëŠ˜ì„ ë‚˜ëŠ” ê³ ì–‘ì´", "style":"realistic", "seed":0 }
"""

import os, uuid
from datetime import datetime
from pathlib import Path
from typing import Optional
import threading
import time
import requests

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from dotenv import load_dotenv

# llama_cpp ëŠ” "ë¡œì»¬ ëª¨ë¸ fallback" ìš©ìœ¼ë¡œë§Œ ë‚¨ê²¨ë‘  (remote ëª¨ë“œë©´ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
try:
    from llama_cpp import Llama
    GGUF_AVAILABLE = True
except ImportError:
    GGUF_AVAILABLE = False

# ---- env ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env) ----
ROOT_DIR = Path(__file__).resolve().parents[2]  # .../hidden-leaf-village
load_dotenv(dotenv_path=ROOT_DIR / ".env", override=True)

router = APIRouter()

# Renderì—ì„œ ì •ì  íŒŒì¼ì´ ì„œë¹„ìŠ¤ë˜ëŠ” ê³µê°œ URL (Render í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ë””í´íŠ¸)
BACKEND_PUBLIC_URL = os.getenv("BACKEND_PUBLIC_URL", "https://hidden-leaf-village.onrender.com").rstrip("/")

# ì €ì¥ ë£¨íŠ¸ (Render íŒŒì¼ì‹œìŠ¤í…œ)
STORAGE_ROOT = os.getenv(
    "STORAGE_ROOT",
    os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "data"))
)
OUTPUT_DIR = os.path.join(STORAGE_ROOT, "outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ==========================
# ğŸ”— ì›ê²©(ngrok) ì—”ë“œí¬ì¸íŠ¸
# ==========================
# 1) ë¡œì»¬ ComfyUI ngrok ì£¼ì†Œ
COMFYUI_URL = "https://nonblamable-timothy-superattainable.ngrok-free.dev"

# 2) ë¡œì»¬ ë²ˆì—­ ë¸Œë¦¿ì§€ ngrok ì£¼ì†Œ (ì¤‘ìš”: ì´ê²Œ ìˆìœ¼ë©´ 'remote' ëª¨ë“œë¡œ ë™ì‘)
TRANSLATION_BRIDGE_URL = os.getenv("TRANSLATION_BRIDGE_URL", "https://YOUR-TRANSLATION-NGROK-URL").rstrip("/")

# ComfyUIì—ì„œ ê¸°ëŒ€í•˜ëŠ” ëª¨ë¸ íŒŒì¼ëª… (ComfyUI ì¸¡ models í´ë”ì— ì¤€ë¹„)
COMFYUI_MODELS = {
    "unet": "flux1-schnell-Q4_K_S.gguf",
    "clip_l": "clip_l.safetensors", 
    "clip_t5": "t5xxl_fp16.safetensors",
    "vae": "ae.safetensors"
}

# === ë¡œì»¬ ëª¨ë¸ fallback (Renderì—ì„  ì‚¬ì‹¤ìƒ ì‚¬ìš© ë¶ˆê°€ì´ë¯€ë¡œ offê°€ ê¸°ë³¸) ===
# Hugging Face Hub ê²½ë¡œë¥¼ ë‚¨ê²¨ë‘ì§€ë§Œ, Render ë©”ëª¨ë¦¬ í•œê³„ ë•Œë¬¸ì— remote ëª¨ë“œê°€ ê¸°ë³¸ì…ë‹ˆë‹¤.
HF_REPO_ID = "Chloros/rosetta-12b-gguf"
HF_FILENAME = "yanolja_rosetta_12b_q4_k_m.gguf"

# ì—ëŸ¬ ë©”ì‹œì§€ ìƒìˆ˜ ì •ì˜
class ErrorMessages:
    # 400 Bad Request
    TEXT_TOO_LONG = "í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ 1000ìë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤."
    TEXT_EMPTY = "ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
    INVALID_SEED = "seed ê°’ì€ 0 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤."
    MALFORMED_REQUEST = "ìš”ì²­ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
    
    # 500 Internal Server Error
    CONFIG_ERROR = "ì„œë²„ ì„¤ì • ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    FILE_SAVE_ERROR = "ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    UNKNOWN_ERROR = "ì•Œ ìˆ˜ ì—†ëŠ” ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    MODEL_LOAD_ERROR = "ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    MODEL_MISSING_ERROR = "í•„ìš”í•œ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
    
    # 502 Bad Gateway
    TRANSLATION_ERROR = "ë²ˆì—­ ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    IMAGE_GENERATION_ERROR = "ì´ë¯¸ì§€ ìƒì„± ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


class CopyToImageReq(BaseModel):
    text: str
    style: Optional[str] = None
    seed: Optional[int] = None


# ê¸€ë¡œë²Œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
_translator = None
_model_loading_lock = threading.Lock()


class LocalModelPipeline:
    """
    í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸
    - remote ëª¨ë“œ: Renderì—ì„  ì´ ëª¨ë“œê°€ ê¸°ë³¸. ë¡œì»¬ ë²ˆì—­ ë¸Œë¦¿ì§€(ngrok) + ë¡œì»¬ ComfyUI(ngrok)ì— HTTPë¡œ ë¶™ìŒ.
    - local  ëª¨ë“œ: (ê°œë°œì ë¡œì»¬ì—ì„œë§Œ) llama-cppë¡œ gguf ì§ì ‘ ë¡œë“œ (Render ë©”ëª¨ë¦¬ ì œí•œ ë•Œë¬¸ì— ì‹¤ì„œë¹„ìŠ¤ì—ì„  ë¹„ê¶Œì¥)
    """
    def __init__(self):
        self.translator = None
        self.loaded = False

        # remote ëª¨ë“œ ì—¬ë¶€
        self.remote_translation = bool(TRANSLATION_BRIDGE_URL and TRANSLATION_BRIDGE_URL.startswith("http"))
    
    # ---------- ê³µìš© í—¬í¼ ----------
    def _http_post_json(self, url: str, payload: dict, timeout=15):
        try:
            r = requests.post(url, json=payload, timeout=timeout)
            if r.status_code >= 400:
                raise HTTPException(502, f"Upstream error {r.status_code}: {r.text[:300]}")
            return r.json()
        except requests.RequestException as e:
            raise HTTPException(502, f"Upstream request failed: {e}")

    # ---------- ì…‹ì—…/ì²´í¬ ----------
    def check_models(self):
        """
        ì›ê²© ëª¨ë“œ: ë²ˆì—­ ë¸Œë¦¿ì§€ health, ComfyUI ì—°ê²°ë§Œ í™•ì¸
        ë¡œì»¬ ëª¨ë“œ: gguf íŒŒì¼ ì¡´ì¬, llama-cpp ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸
        """
        # 1) ë²ˆì—­ ë¸Œë¦¿ì§€(ì›ê²©) ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ ì²´í¬
        if self.remote_translation:
            # /health ë˜ëŠ” /translate ê°„ë‹¨ í˜¸ì¶œë¡œ í™•ì¸
            try:
                ping = self._http_post_json(f"{TRANSLATION_BRIDGE_URL}/translate", {"text":"ì•ˆë…•"}, timeout=8)
                if not isinstance(ping, dict) or "text" not in ping:
                    raise HTTPException(502, f"{ErrorMessages.TRANSLATION_ERROR}: ë¸Œë¦¿ì§€ ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜")
            except HTTPException:
                raise
            except Exception as e:
                raise HTTPException(502, f"{ErrorMessages.TRANSLATION_ERROR}: ë²ˆì—­ ë¸Œë¦¿ì§€ ì—°ê²° ì‹¤íŒ¨ - {e}")
        else:
            # === ë¡œì»¬ ëª¨ë“œ (Renderì—ì„  ì‚¬ì‹¤ìƒ ë¹„í™œì„±) ===
            try:
                from huggingface_hub import hf_hub_download
                model_path = Path(hf_hub_download(repo_id=HF_REPO_ID, filename=HF_FILENAME))
            except Exception as e:
                raise HTTPException(500, f"{ErrorMessages.MODEL_MISSING_ERROR}: HF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - {e}")
            if not GGUF_AVAILABLE:
                raise HTTPException(500, f"{ErrorMessages.CONFIG_ERROR}: llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # 2) ComfyUI ì—°ê²° í™•ì¸
        try:
            response = requests.get(f"{COMFYUI_URL}/system_stats", timeout=5)
            if response.status_code != 200:
                raise HTTPException(
                    status_code=500,
                    detail=f"{ErrorMessages.MODEL_MISSING_ERROR}: ComfyUI ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
        except requests.RequestException:
            raise HTTPException(
                status_code=500,
                detail=f"{ErrorMessages.MODEL_MISSING_ERROR}: ComfyUI ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            )
    
    def load_models(self):
        """
        remote ëª¨ë“œ: ë³„ë„ ë¡œë”© ì—†ìŒ(ë¸Œë¦¿ì§€ pingë§Œ ì„±ê³µí•˜ë©´ ì¤€ë¹„ ì™„ë£Œ)
        local  ëª¨ë“œ: llama-cppë¡œ gguf ë¡œë”© (Renderì—ì„œëŠ” ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ë¹„ê¶Œì¥)
        """
        if self.loaded:
            return
        
        print("íŒŒì´í”„ë¼ì¸ ì²´í¬ ë° ë¡œë”© ì‹œì‘...")
        self.check_models()

        if self.remote_translation:
            print("ì›ê²© ë²ˆì—­ ë¸Œë¦¿ì§€ ëª¨ë“œ: ëª¨ë¸ ë¡œë”© ë¶ˆí•„ìš”")
            self.translator = None
        else:
            # === ë¡œì»¬ ëª¨ë“œ (ê°œë°œì ë¡œì»¬ì—ì„œë§Œ) ===
            from huggingface_hub import hf_hub_download
            model_path = Path(hf_hub_download(repo_id=HF_REPO_ID, filename=HF_FILENAME))
            print(f"ë²ˆì—­ ëª¨ë¸ ë¡œë”©(ë¡œì»¬): {model_path.name}")
            try:
                self.translator = Llama(
                    model_path=str(model_path),
                    n_ctx=512,
                    n_threads=4,
                    n_gpu_layers=-1,
                    verbose=False
                )
            except Exception as e:
                raise HTTPException(
                    status_code=500,
                    detail=f"{ErrorMessages.MODEL_LOAD_ERROR}: ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - {str(e)}"
                )
        
        print("ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ")
        self.loaded = True
    
    # ---------- ë²ˆì—­/ë¶„ë¥˜ ----------
    def translate_korean(self, text: str) -> str:
        """
        í•œê¸€ì„ ì˜ì–´ë¡œ ë²ˆì—­
        - remote ëª¨ë“œ: /translate í˜¸ì¶œ
        - local  ëª¨ë“œ: llama-cpp í˜¸ì¶œ
        """
        if not self.loaded:
            self.load_models()
        
        # í•œê¸€ í¬í•¨ í™•ì¸
        has_korean = any('\uac00' <= char <= '\ud7af' for char in text)
        if not has_korean:
            print(f"í•œê¸€ ì—†ìŒ, ì›ë¬¸ ì‚¬ìš©: {text}")
            return text
        
        if self.remote_translation:
            try:
                resp = self._http_post_json(f"{TRANSLATION_BRIDGE_URL}/translate", {"text": text}, timeout=12)
                english_text = (resp.get("text") or "").strip()
                return english_text or text
            except HTTPException:
                raise
            except Exception as e:
                print(f"ë²ˆì—­ ì˜¤ë¥˜(ë¸Œë¦¿ì§€): {e}, ì›ë¬¸ ì‚¬ìš©")
                return text

        # === ë¡œì»¬ ëª¨ë“œ ===
        print(f"í•œê¸€ ë²ˆì—­ ì¤‘(ë¡œì»¬ llama): {text}")
        prompt = f"""Translate the following Korean text to English:

Korean: {text}
English:"""
        try:
            response = self.translator(
                prompt,
                max_tokens=100,
                temperature=0.0,
                stop=["Korean:", "\n\n", "Translation:"]
            )
            english_text = response['choices'][0]['text'].strip()
            if "English:" in english_text:
                english_text = english_text.split("English:")[-1]
            english_text = english_text.split("\n")[0].strip()
            return english_text or text
        except Exception as e:
            print(f"ë²ˆì—­ ì˜¤ë¥˜(ë¡œì»¬): {e}, ì›ë¬¸ ì‚¬ìš©")
            return text
    
    def classify_person(self, english: str) -> bool:
        """
        ì‚¬ëŒ ì—¬ë¶€ íŒë‹¨ (ëª…ì‹œì  ë‹¨ì–´ í¬í•¨ ì—¬ë¶€)
        - remote ëª¨ë“œ: /classify í˜¸ì¶œ(type=person)
        - local  ëª¨ë“œ: llama-cpp
        """
        if not self.loaded:
            self.load_models()
        
        if self.remote_translation:
            try:
                resp = self._http_post_json(f"{TRANSLATION_BRIDGE_URL}/classify",
                                            {"text": english, "type": "person"},
                                            timeout=6)
                return bool(resp.get("yes", False))
            except HTTPException:
                raise
            except Exception as e:
                print(f"ì¸ë¬¼ íŒë‹¨ ì˜¤ë¥˜(ë¸Œë¦¿ì§€): {e}")
                return False

        # === ë¡œì»¬ ëª¨ë“œ ===
        prompt = f"""Answer only YES or NO.

Text: {english}

Question: Does this text explicitly contain any human-related words 
(such as man, woman, person, people, child, boy, girl, baby, face, portrait, model, actor, actress, selfie)?

Rules:
- Answer YES only if at least one of these words appears in the text.
- If none of these words appear, you MUST answer NO.
- Do not assume or guess implied humans (e.g., someone riding a bicycle).
- Do not use context or imagination. Base your answer only on explicit words in the text.

Answer:"""
        try:
            response = self.translator(prompt, max_tokens=5, temperature=0.0, stop=["\n"])
            answer = response['choices'][0]['text'].strip().lower()
            return "yes" in answer
        except Exception as e:
            print(f"ì¸ë¬¼ íŒë‹¨ ì˜¤ë¥˜(ë¡œì»¬): {e}")
            return False
    
    def classify_object(self, english: str) -> bool:
        """
        ì‚¬ë¬¼ ì—¬ë¶€ íŒë‹¨ (ëª…ì‹œì  ë‹¨ì–´ í¬í•¨ ì—¬ë¶€) â€” ì˜ë„ìƒ 'ì‚¬ëŒ ë‹¨ì–´'ê°€ ìˆìœ¼ë©´ YESë¡œ í•˜ë˜ ì›ë˜ ë²„ê·¸ì„± ê·œì¹™ì„ ê·¸ëŒ€ë¡œ ìœ ì§€
        - remote ëª¨ë“œ: /classify(type=object) í˜¸ì¶œ (ë™ì¼ ê·œì¹™ì„ ë¸Œë¦¿ì§€ì—ì„œ êµ¬í˜„)
        - local  ëª¨ë“œ: llama-cpp
        """
        if not self.loaded:
            self.load_models()
        
        if self.remote_translation:
            try:
                resp = self._http_post_json(f"{TRANSLATION_BRIDGE_URL}/classify",
                                            {"text": english, "type": "object"},
                                            timeout=6)
                return bool(resp.get("yes", False))
            except HTTPException:
                raise
            except Exception as e:
                print(f"ì‚¬ë¬¼ íŒë‹¨ ì˜¤ë¥˜(ë¸Œë¦¿ì§€): {e}")
                return False

        # === ë¡œì»¬ ëª¨ë“œ === (ì›ë³¸ ê·œì¹™ ìœ ì§€)
        prompt = f"""Answer with only YES or NO.

Text: {english}

Rule:
- Answer YES only if the text explicitly mentions humans or human-related words.
- Do NOT infer implied presence.
- If unclear, answer NO.

Answer:"""
        try:
            response = self.translator(prompt, max_tokens=5, temperature=0.0, stop=["\n"])
            answer = response['choices'][0]['text'].strip().lower()
            return "yes" in answer
        except Exception as e:
            print(f"ì‚¬ë¬¼ íŒë‹¨ ì˜¤ë¥˜(ë¡œì»¬): {e}")
            return False
    
    def enhance_prompt(self, text: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ê°•í™”: ë²ˆì—­ + ë¶„ë¥˜ + í‚¤ì›Œë“œ ì¶”ê°€ (ì›ë³¸ ë¡œì§ ìœ ì§€)"""
        if not self.loaded:
            self.load_models()
        
        print("\n=== í”„ë¡¬í”„íŠ¸ ê°•í™” ì‹œì‘ ===")
        
        # 1. ë²ˆì—­
        english = self.translate_korean(text)
        
        # 2. ë¶„ë¥˜
        print("ì½˜í…ì¸  ë¶„ë¥˜ ì¤‘...")
        has_person = self.classify_person(english)
        has_object = self.classify_object(english)
        
        # 3. í‚¤ì›Œë“œ ê°•í™”
        enhanced = f"{english}, sharp, clean composition, high quality"
        
        if has_person:
            enhanced += ", portrait, detailed face, natural skin texture"
            print("  ì¸ë¬¼ í‚¤ì›Œë“œ ì¶”ê°€")
        
        if has_object:
            enhanced += ", sharp edges"
            print("  ì‚¬ë¬¼ í‚¤ì›Œë“œ ì¶”ê°€")
        
        print(f"ìµœì¢… ê°•í™” í”„ë¡¬í”„íŠ¸: {enhanced}")
        print("=== í”„ë¡¬í”„íŠ¸ ê°•í™” ì™„ë£Œ ===\n")
        
        return enhanced
    
    def generate_image_with_comfyui(self, prompt: str, style: Optional[str] = None, seed: Optional[int] = None) -> bytes:
        """ComfyUI(ngrok)ë¡œ ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± (ì›ë³¸ ì›Œí¬í”Œë¡œìš° ìœ ì§€)"""
        print(f"ComfyUIë¡œ ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„±: {prompt}")
        
        # ComfyUI ì›Œí¬í”Œë¡œìš° ì •ì˜ (ê²€ì¦ëœ êµ¬ì¡° ì‚¬ìš©)
        workflow = {
            "1": {
                "inputs": {"unet_name": COMFYUI_MODELS["unet"]},
                "class_type": "UnetLoaderGGUF",
                "_meta": {"title": "Load GGUF Model"}
            },
            "2": {
                "inputs": {
                    "clip_name1": COMFYUI_MODELS["clip_l"],
                    "clip_name2": COMFYUI_MODELS["clip_t5"],
                    "type": "flux"
                },
                "class_type": "DualCLIPLoader",
                "_meta": {"title": "Load CLIP"}
            },
            "3": {
                "inputs": {
                    "text": f"{prompt} in {style} style" if style else prompt,
                    "clip": ["2", 0]
                },
                "class_type": "CLIPTextEncode",
                "_meta": {"title": "Encode Prompt"}
            },
            "4": {
                "inputs": {
                    "width": 512,
                    "height": 512,
                    "batch_size": 1
                },
                "class_type": "EmptyLatentImage",
                "_meta": {"title": "Empty Latent"}
            },
            "5": {
                "inputs": {
                    "seed": seed or int(time.time()) % 1000000,
                    "steps": 4,
                    "cfg": 1.0,
                    "sampler_name": "euler",
                    "scheduler": "simple",
                    "denoise": 1.0,
                    "model": ["1", 0],
                    "positive": ["3", 0],
                    "negative": ["3", 0],
                    "latent_image": ["4", 0]
                },
                "class_type": "KSampler",
                "_meta": {"title": "Sample"}
            },
            "6": {
                "inputs": {"vae_name": COMFYUI_MODELS["vae"]},
                "class_type": "VAELoader",
                "_meta": {"title": "Load VAE"}
            },
            "7": {
                "inputs": {
                    "samples": ["5", 0],
                    "vae": ["6", 0]
                },
                "class_type": "VAEDecode",
                "_meta": {"title": "Decode"}
            },
            "8": {
                "inputs": {
                    "filename_prefix": "flux_output",
                    "images": ["7", 0]
                },
                "class_type": "SaveImage",
                "_meta": {"title": "Save"}
            }
        }
        
        try:
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            client_id = str(uuid.uuid4())
            
            response = requests.post(
                f"{COMFYUI_URL}/prompt",
                json={"prompt": workflow, "client_id": client_id},
                timeout=12
            )
            
            if response.status_code != 200:
                error_detail = ""
                try:
                    error_detail = response.json()
                except:
                    error_detail = response.text
                raise Exception(f"ComfyUI ìš”ì²­ ì‹¤íŒ¨: {response.status_code} - {error_detail}")
            
            prompt_id = response.json()["prompt_id"]
            print(f"ComfyUI ì‘ì—… ID: {prompt_id}")
            
            # ì™„ë£Œ ëŒ€ê¸°
            for _ in range(150):  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                time.sleep(2)
                
                hist_response = requests.get(f"{COMFYUI_URL}/history/{prompt_id}", timeout=8)
                if hist_response.status_code == 200:
                    history = hist_response.json()
                    
                    if prompt_id in history:
                        task_info = history[prompt_id]
                        status = task_info.get("status", {})
                        
                        if status.get("completed", False):
                            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                            outputs = task_info.get("outputs", {})
                            for node_id, output in outputs.items():
                                if "images" in output:
                                    for img_info in output["images"]:
                                        img_url = f"{COMFYUI_URL}/view"
                                        params = {
                                            "filename": img_info["filename"],
                                            "subfolder": img_info.get("subfolder", ""),
                                            "type": "output"
                                        }
                                        
                                        img_response = requests.get(img_url, params=params, timeout=12)
                                        if img_response.status_code == 200:
                                            print("ComfyUI ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                                            return img_response.content
                        
                        elif "error" in status:
                            raise Exception(f"ComfyUI ì˜¤ë¥˜: {status['error']}")
            
            raise Exception("ComfyUI íƒ€ì„ì•„ì›ƒ")
            
        except Exception as e:
            # ComfyUI ì‹¤íŒ¨ì‹œ ë°ëª¨ ëª¨ë“œë¡œ fallback
            print(f"ComfyUI ì‹¤íŒ¨, ë°ëª¨ ëª¨ë“œë¡œ fallback: {e}")
            return self.generate_image_demo(prompt, style, seed)
    
    def generate_image_demo(self, prompt: str, style: Optional[str] = None, seed: Optional[int] = None) -> bytes:
        """ë°ëª¨ ì´ë¯¸ì§€ ìƒì„± (ComfyUI ì‹¤íŒ¨ì‹œ fallback)"""
        from PIL import Image, ImageDraw, ImageFont
        import io
        
        print(f"ë°ëª¨ ì´ë¯¸ì§€ ìƒì„± (fallback): {prompt}")
        
        # 1024x1024 ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (1024, 1024), color='lightcoral')
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        title_font = None
        try:
            title_font = ImageFont.load_default()
        except:
            pass
        
        # ì œëª© í…ìŠ¤íŠ¸
        title = "ComfyUI Connection Failed - Demo Mode"
        if title_font:
            bbox = draw.textbbox((0, 0), title, font=title_font)
            title_width = bbox[2] - bbox[0]
            x_pos = (1024 - title_width) // 2
            draw.text((x_pos, 100), title, fill='white', font=title_font)
        
        # í”„ë¡¬í”„íŠ¸ ì •ë³´
        info_lines = [
            f"Prompt: {prompt[:60]}{'...' if len(prompt) > 60 else ''}",
            f"Style: {style or 'None'}",
            f"Seed: {seed or 'Random'}",
            "",
            "ComfyUI Status: Failed (ngrok?)",
            f"Check: {COMFYUI_URL}",
            "",
            "Translation: remote bridge",
            f"Bridge: {TRANSLATION_BRIDGE_URL}"
        ]
        
        y_offset = 300
        for line in info_lines:
            if title_font:
                bbox = draw.textbbox((0, 0), line, font=title_font)
                line_width = bbox[2] - bbox[0]
                x_pos = (1024 - line_width) // 2
                draw.text((x_pos, y_offset), line, fill='white', font=title_font)
            y_offset += 35
        
        # PIL ì´ë¯¸ì§€ë¥¼ bytesë¡œ ë³€í™˜
        img_bytes = io.BytesIO()
        img.save(img_bytes, format='PNG')
        return img_bytes.getvalue()


def _get_local_pipeline():
    """íŒŒì´í”„ë¼ì¸ ì‹±ê¸€í†¤"""
    global _translator
    
    if _translator is None:
        with _model_loading_lock:
            if _translator is None:
                _translator = LocalModelPipeline()
    
    return _translator


def _validate_request(req: CopyToImageReq) -> CopyToImageReq:
    """ê¸°ë³¸ ìš”ì²­ ê²€ì¦"""
    try:
        # ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬
        if not hasattr(req, 'text') or req.text is None:
            raise HTTPException(
                status_code=400, 
                detail=ErrorMessages.MALFORMED_REQUEST
            )
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì²´í¬ (400 Bad Request)
        if len(req.text) > 1000:
            raise HTTPException(
                status_code=400, 
                detail=ErrorMessages.TEXT_TOO_LONG
            )
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬
        if not req.text.strip():
            raise HTTPException(
                status_code=400, 
                detail=ErrorMessages.TEXT_EMPTY
            )
        
        # seed ê²€ì¦
        if req.seed is not None and (not isinstance(req.seed, int) or req.seed < 0):
            raise HTTPException(
                status_code=400, 
                detail=ErrorMessages.INVALID_SEED
            )
        
        return req
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"{ErrorMessages.MALFORMED_REQUEST}: {str(e)}"
        )


@router.post("/image-from-copy")
def image_from_copy(req: CopyToImageReq):
    """í…ìŠ¤íŠ¸ë¡œë¶€í„° ì´ë¯¸ì§€ ìƒì„± - í”„ë¡¬í”„íŠ¸ ê°•í™” ì ìš©"""
    
    # ê¸°ë³¸ ìš”ì²­ ê²€ì¦
    validated_req = _validate_request(req)
    
    start_time = time.time()
    
    try:
        pipeline = _get_local_pipeline()
        
        # 1. í”„ë¡¬í”„íŠ¸ ê°•í™” (ë²ˆì—­ + ë¶„ë¥˜ + í‚¤ì›Œë“œ ì¶”ê°€)
        enhancement_start = time.time()
        enhanced_prompt = pipeline.enhance_prompt(validated_req.text)
        enhancement_time = time.time() - enhancement_start
        
        # 2. ìŠ¤íƒ€ì¼ ì ìš© (ì„ íƒì‚¬í•­)
        if validated_req.style:
            final_prompt = f"{enhanced_prompt} in {validated_req.style} style"
        else:
            final_prompt = enhanced_prompt
        
        # 3. ComfyUIë¡œ ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„±
        generation_start = time.time()
        img_bytes = pipeline.generate_image_with_comfyui(
            final_prompt, 
            validated_req.style, 
            validated_req.seed
        )
        generation_time = time.time() - generation_start
        
        # 4. íŒŒì¼ ì €ì¥ (Render íŒŒì¼ì‹œìŠ¤í…œ â†’ Render ì •ì  URLë¡œ ì ‘ê·¼)
        save_name = f"image_from_copy_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}.png"
        save_path = os.path.join(OUTPUT_DIR, save_name)
        
        with open(save_path, "wb") as f:
            f.write(img_bytes)
            
        file_path = os.path.abspath(save_path).replace("\\", "/")
        file_url = f"{BACKEND_PUBLIC_URL}/static/outputs/{save_name}"
        
        total_time = time.time() - start_time
        
        return {
            "ok": True, 
            "output_path": file_path, 
            "file_url": file_url,
            "metadata": {
                "original_text": validated_req.text,
                "enhanced_prompt": enhanced_prompt,
                "final_prompt": final_prompt,
                "style": validated_req.style,
                "seed": validated_req.seed,
                "model_used": "ComfyUI(remote) + TranslationBridge(remote)",
                "demo_mode": False,
                "timing": {
                    "enhancement_time": round(enhancement_time, 2),
                    "generation_time": round(generation_time, 2),
                    "total_time": round(total_time, 2)
                }
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"{ErrorMessages.UNKNOWN_ERROR}: {str(e)}"
        )


# ëª¨ë¸ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
@router.get("/model-status")
def model_status():
    """í˜„ì¬ ëª¨ë¸/ë¸Œë¦¿ì§€/ComfyUI ìƒíƒœ í™•ì¸ (remote ìš°ì„ )"""
    
    # ë²ˆì—­(ë¸Œë¦¿ì§€) ì²´í¬
    bridge_ok = False
    try:
        # ê°„ë‹¨ ping
        ping = requests.post(f"{TRANSLATION_BRIDGE_URL}/translate", json={"text": "í…ŒìŠ¤íŠ¸"}, timeout=5)
        bridge_ok = (ping.status_code == 200 and "text" in (ping.json() or {}))
    except Exception:
        bridge_ok = False
    
    # ComfyUI ì—°ê²° ì²´í¬
    comfyui_available = False
    comfyui_models = {}
    
    try:
        response = requests.get(f"{COMFYUI_URL}/system_stats", timeout=5)
        if response.status_code == 200:
            comfyui_available = True
            
            # ëª¨ë¸ ëª©ë¡ í™•ì¸
            obj_info_response = requests.get(f"{COMFYUI_URL}/object_info", timeout=5)
            if obj_info_response.status_code == 200:
                obj_info = obj_info_response.json()
                
                # GGUF ëª¨ë¸ ì²´í¬
                if "UnetLoaderGGUF" in obj_info:
                    unet_models = obj_info["UnetLoaderGGUF"]["input"]["required"]["unet_name"][0]
                    comfyui_models["unet"] = COMFYUI_MODELS["unet"] in unet_models
                
                # CLIP ëª¨ë¸ ì²´í¬  
                if "DualCLIPLoader" in obj_info:
                    clip_models = obj_info["DualCLIPLoader"]["input"]["required"]["clip_name1"][0]
                    comfyui_models["clip_l"] = COMFYUI_MODELS["clip_l"] in clip_models
                    comfyui_models["clip_t5"] = COMFYUI_MODELS["clip_t5"] in clip_models
                
                # VAE ëª¨ë¸ ì²´í¬
                if "VAELoader" in obj_info:
                    vae_models = obj_info["VAELoader"]["input"]["required"]["vae_name"][0]
                    comfyui_models["vae"] = COMFYUI_MODELS["vae"] in vae_models
    
    except:
        pass
    
    all_models_ready = bridge_ok and comfyui_available and all(comfyui_models.values()) if comfyui_models else (bridge_ok and comfyui_available)
    
    return {
        "translation_bridge": {
            "url": TRANSLATION_BRIDGE_URL,
            "reachable": bridge_ok
        },
        "comfyui": {
            "server_available": comfyui_available,
            "url": COMFYUI_URL,
            "models": comfyui_models,
            "expected_models": COMFYUI_MODELS
        },
        "dependencies": {
            "mode": "remote" if TRANSLATION_BRIDGE_URL else "local-fallback",
            "gguf_available": GGUF_AVAILABLE
        },
        "prompt_enhancement": {
            "base_quality": "sharp, clean composition, high quality",
            "person_keywords": "portrait, detailed face, natural skin texture",
            "object_keywords": "product photo, centered object, sharp edges"
        },
        "status": "ready" if all_models_ready else "not_ready",
        "message": "ëª¨ë“  ì‹œìŠ¤í…œ ì¤€ë¹„ë¨" if all_models_ready else "ë¸Œë¦¿ì§€/ComfyUI ì—°ê²° í™•ì¸ í•„ìš”"
    }
